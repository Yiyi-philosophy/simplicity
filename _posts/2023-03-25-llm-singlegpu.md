---
title: 'Read List of LLM'
image: '2023-03-25-llm-singlegpu/1679629332417.gif'
tags: Explore LLM
---
<!--more-->

# Sourse

## Useful blog

- ğŸ”²[AIç®—åŠ›çš„é˜¿å–€ç‰æ–¯ä¹‹è¸µï¼šå†…å­˜å¢™ - OneFlowçš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/363041668)
> ![1680183311897](../images/2023-03-25-llm-singlegpu/1680183311897.png)
> ![1680183328626](../images/2023-03-25-llm-singlegpu/1680183328626.png)

- âœ…[A Summary Thread on Attention in Transformers](https://mem.ai/p/yYEcOhYFjmjq32CtzEPX)

- âœ…[How to Estimate the Number of Parameters in Transformer models](https://towardsdatascience.com/how-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0)

- ğŸ”²[OpenAI å·¥ç¨‹å¸ˆ lilianweng å…³äºLLMæ¨ç†è®¡ç®—ä¼˜åŒ–çš„æ–¹æ³•æ¦‚è¿°](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)

## [ç‹ç‡•é£](https://www.zhihu.com/people/nono-nono-66/posts)

- âœ…[å¤§æ¨¡å‹LLMsç®—æ³•å’Œè®¡ç®—ç³»ç»Ÿé‡ç‚¹è®ºæ–‡ - AIç³»ç»Ÿå·¥ç¨‹å¸ˆè§†è§’Reading List ï¼ˆ1ï¼‰ - ç‹ç‡•é£çš„æ–‡ç« ](https://zhuanlan.zhihu.com/p/608268806)

- âœ…[å¤§æ¨¡å‹LLMsç®—æ³•å’Œè®¡ç®—ç³»ç»Ÿé‡ç‚¹è®ºæ–‡ - AIç³»ç»Ÿå·¥ç¨‹å¸ˆè§†è§’Reading List ï¼ˆ2ï¼‰ - ç‹ç‡•é£çš„æ–‡ç« ](https://zhuanlan.zhihu.com/p/616200081)

### Flexgen

- âœ…[å•å¡é«˜ååçš„å¤§è¯­è¨€æ¨¡å‹æ¨ç† - å˜»å˜»å˜»çš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/608279479)

- âœ…[Flexgen LLMæ¨ç† CPU Offloadè®¡ç®—æ¶æ„åˆ°åº•å¹²äº†ä»€ä¹ˆäº‹æƒ…ï¼Ÿ - ç‹ç‡•é£çš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/615021309)

- âœ…[Flexgen LLMæ¨ç†è®¡ç®—ç¯èŠ‚çš„é‡åŒ–åˆ†æ - ç‹ç‡•é£çš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/615327112)

- âœ…[Flexgen LLMæ¨ç†ç›¸å…³å·¥ä½œ - æ€ä¹ˆæ€è€ƒå¯»æ‰¾ä¼˜åŒ–æ–¹æ³• - ç‹ç‡•é£çš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/615328081)

- ğŸ”²[DeepSpeedä¹‹ZeROç³»åˆ—ï¼šå°†æ˜¾å­˜ä¼˜åŒ–è¿›è¡Œåˆ°åº• - basicv8vcçš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/513571706)

## Muli

### Methods

- âœ…[**å¦‚ä½•è¯»è®ºæ–‡**ï¼š](https://www.bilibili.com/video/BV1H44y1t75x/)

- ğŸ”²[å¦‚ä½•åˆ¤æ–­ï¼ˆä½ è‡ªå·±çš„ï¼‰ç ”ç©¶å·¥ä½œçš„ä»·å€¼- [è®ºæ–‡ç²¾è¯»]](https://www.bilibili.com/video/BV1oL411c7Us/)

- ğŸ”²[ä½ ï¼ˆè¢«ï¼‰åæ§½è¿‡è®ºæ–‡ä¸å¤Ÿ novel å—ï¼Ÿ- [è®ºæ–‡ç²¾è¯»]](https://www.bilibili.com/video/BV1ea41127Bq/)

### Background Knowledge

- âœ…[Transformerè®ºæ–‡é€æ®µç²¾è¯»- [è®ºæ–‡ç²¾è¯»]](https://www.bilibili.com/video/BV1pu411o7BE/)

- ğŸ”²[BERT è®ºæ–‡é€æ®µç²¾è¯»ã€è®ºæ–‡ç²¾è¯»ã€‘](https://www.bilibili.com/video/BV1PL411M7eQ/)

- âœ…[GPTï¼ŒGPT-2ï¼ŒGPT-3 è®ºæ–‡ç²¾è¯»- [è®ºæ–‡ç²¾è¯»]](https://www.bilibili.com/video/BV1AF411b7xQ/)

- ğŸ”²[Megatron LM è®ºæ–‡ç²¾è¯»- [è®ºæ–‡ç²¾è¯»]](https://www.bilibili.com/video/BV1nB4y1R7Yz/)

- ğŸ”²[Zero è®ºæ–‡ç²¾è¯»[è®ºæ–‡ç²¾è¯»]](https://www.bilibili.com/video/BV1tY411g7ZT/)

- ğŸ”²[InstructGPT è®ºæ–‡ç²¾è¯»[è®ºæ–‡ç²¾è¯»Â·48]](https://www.bilibili.com/video/BV1hd4y187CR/)

- ğŸ”²[HELM å…¨é¢è¯­è¨€æ¨¡å‹è¯„æµ‹[è®ºæ–‡ç²¾è¯»Â·50]](https://www.bilibili.com/video/BV1z24y1B7uX/)

- ğŸ”²[Anthropic LLM è®ºæ–‡ç²¾è¯»[è®ºæ–‡ç²¾è¯»Â·51]](https://www.bilibili.com/video/BV1XY411B7nM/)

----

# **Article**

- âœ…[How to Read a Paper](https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf)

> å› ä¸ºLLMåº”ç”¨æ•ˆæœæ˜¾è‘—ï¼ˆOpenAI chatGPT/GPT-4ã€meta OPT/LLaMAï¼‰ï¼Œæ‰€ä»¥LLMçš„è®¡ç®—ç³»ç»Ÿç ”ç©¶å·¥ä½œä»ä¸“æ³¨è®­ç»ƒç³»ç»Ÿè®¾è®¡åˆ°æ¨ç†ç³»ç»Ÿæ–¹å‘æ‹“å±•ï¼Œç›®æ ‡é™ä½LLMæ¨ç†æˆæœ¬ã€æ¨ç†é—¨æ§›ï¼ŒFlexgenæåˆ°äº†å‡ ä¸ªé‡è¦å·¥ä½œï¼Œè°·æ­Œä»£è¡¨æ€§å·¥ä½œ **PaLM inferenc**e ã€å¾®è½¯ä»£è¡¨æ€§å·¥ä½œ   **Deepspeed-Inference**ï¼Œå¦å¤–OSDI22å¹´ **Orca**å·¥ä½œã€‚PaLM inferenceå’ŒDeepspeed-Inferenceæ˜¯ç«¯åˆ°ç«¯ç³»ç»Ÿå¹¶è¡Œç³»ç»Ÿè®¾è®¡ï¼›Orcaä¾§é‡å¯¹å˜æˆseqçš„è®¡ç®—æ•ˆç‡æå‡ï¼Œè®¾è®¡token levelçš„å¹¶è¡Œè®¡ç®—ç³»ç»Ÿï¼Œä¾§é‡ç³»ç»Ÿè®¾è®¡ï¼ˆä¸å•çº¯æ˜¯å¤šGPUåŒæ„ç³»ç»Ÿä¸Šçš„é«˜æ€§èƒ½è®¡ç®—çš„å¹¶è¡Œè®¾è®¡ï¼Œæ›´å¼ºè°ƒç³»ç»ŸåŒ–çš„è®¾è®¡ï¼ŒåŒ…å«æ¨ç†è®¡ç®—ã€è°ƒåº¦æœåŠ¡ï¼‰æ¥æå‡æ¨ç†è®¡ç®—çš„èµ„æºåˆ©ç”¨ç‡ã€‚å…¶ä»–è¿˜æœ‰ï¼ŒFasterTransformerã€LightSeqã€TurboTransformers ã€  Huggingfaceçš„Accelerateå·¥ä½œã€‚è¿™ç±»å·¥ä½œæ¯”è¾ƒä¾§é‡ä»è®¡ç®—æœºç³»ç»Ÿè®¾è®¡è§’åº¦ç ”ç©¶å¯¹LLMæ¨ç†è®¡ç®—çš„ä¼˜åŒ–ï¼Œå¤§éƒ¨åˆ†ä¼˜åŒ–å…·æœ‰æ— æŸç‰¹æ€§ï¼Œç›¸å¯¹é€šç”¨æ€§æ›´é«˜ï¼ˆä½†æ˜¯éƒ¨åˆ†å·¥ä½œé‡Œä¹Ÿå­˜åœ¨æ­£äº¤çš„æœ‰æŸä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿç³»ç»Ÿæ€§èƒ½æå‡ï¼‰ã€‚

## **PaLM inference**
- ğŸ”²[Blog](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)
![1679629332417](../images/2023-03-25-llm-singlegpu/1679629332417.gif)
- ğŸ”²[PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf)

> æ¨¡å‹å±‚é¢ä¸»è¦æœ‰å¦‚ä¸‹æ”¹åŠ¨ï¼šä½¿ç”¨SwiGLUæ¿€æ´»å‡½æ•°ï¼Œæœ‰ç ”ç©¶è¯æ˜åœ¨åŒç­‰è®¡ç®—é‡ä¸‹SwiGLUçš„æ•ˆæœæ›´å¥½æŠŠFFNå’ŒAttentionå¹¶è¡Œ 

> ä»¥å‰ï¼š$y=x+MLP(LayerNorm(x+Attention(LayerNorm(x)))$
>
> ç°åœ¨ï¼š$y = x + MLP(LayerNorm(x)) + Attention(LayerNorm(x))$
>
>  **Multi-Query Attentionï¼š**ä»¥å¾€åšattentionå‰æˆ‘ä»¬éƒ½ä¼šæŠŠQå’ŒKéšå±‚æ˜ å°„åˆ°[head_num, head_size]ï¼Œè€ŒPaLMè®©æ‰€æœ‰å¤´å…±äº«å‚æ•°çŸ©é˜µï¼Œåªæ˜ å°„åˆ°[1, head_size],å¯¹è®­ç»ƒé€Ÿåº¦å’Œæ•ˆæœæ²¡ä»€ä¹ˆå½±å“ï¼Œä½†å´æå‡äº†decodeçš„é€Ÿåº¦ä½¿ç”¨RoPE[2]ä½ç½®ç¼–ç ï¼š
> 
> RoPEæ˜¯è‹ç¥çš„å·¥ä½œï¼Œä¸»è¦åˆ©ç”¨ä¸‰è§’å‡½æ•°çš„æ’ç­‰å˜æ¢æ¥ä¼˜åŒ–ç›¸å¯¹ä½ç½®ç¼–ç è¾“å…¥å’Œè¾“å‡ºå…±äº«embeddingçŸ©é˜µå»æ‰æ‰€æœ‰çš„Biasé¡¹ä½¿ç”¨256Kä¸ªtokençš„SentencePiece

> ä½œè€…åœ¨introä¸­ä¹Ÿæ€»ç»“äº†å¤§æ¨¡å‹æå‡çš„å¥—è·¯ï¼š
> 
> - æ‰©å¤§æ¨¡å‹æ·±åº¦å’Œå®½åº¦
> - å¢åŠ è®­ç»ƒçš„tokenæ•°ç›®
> - æ›´å¹²å‡€å’Œæ›´å¤šæºçš„æ•°æ®
> - åˆ©ç”¨ç¨€ç–è®¡ç®—å¢åŠ æ¨¡å‹å®¹é‡

## **Deepspeed-Inference**
- ğŸ”²[https://www.deepspeed.ai/inference/](https://www.deepspeed.ai/inference/)
DeepSpeed Inference: Enabling Efficient Inference
of Transformer Models at Unprecedented Scale
- ğŸ”²[https://arxiv.org/pdf/2207.00032.pdf](https://arxiv.org/pdf/2207.00032.pdf)

### ZeRO
- ğŸ”²[DeepSpeedä¹‹ZeROç³»åˆ—ï¼šå°†æ˜¾å­˜ä¼˜åŒ–è¿›è¡Œåˆ°åº• - basicv8vcçš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/513571706)
> Offloadç­–ç•¥
> ZeRO-Offloadå¹¶ä¸å¸Œæœ›ä¸ºäº†æœ€å°åŒ–æ˜¾å­˜å ç”¨è€Œè®©ç³»ç»Ÿçš„è®¡ç®—æ•ˆç‡ä¸‹é™.ä½†æ˜¯å°†éƒ¨åˆ†GPUçš„è®¡ç®—å’Œå­˜å‚¨ä¸‹æ”¾åˆ°CPUå’Œå†…å­˜ï¼Œå¿…ç„¶æ¶‰åŠCPUå’ŒGPUä¹‹é—´çš„é€šä¿¡å¢åŠ ï¼Œ**ä¸èƒ½è®©é€šä¿¡æˆä¸ºç“¶é¢ˆ**ï¼Œæ­¤å¤–GPUçš„è®¡ç®—æ•ˆç‡ç›¸æ¯”äºCPUä¹Ÿæ˜¯æ•°é‡çº§ä¸Šçš„ä¼˜åŠ¿ï¼Œ**ä¹Ÿä¸èƒ½è®©CPUå‚ä¸è¿‡å¤šè®¡ç®—**ï¼Œé¿å…æˆä¸ºç³»ç»Ÿç“¶é¢ˆï¼Œåªæœ‰å‰ä¸¤æ¡æ»¡è¶³çš„å‰æä¸‹ï¼Œå†è€ƒè™‘æœ€å°åŒ–æ˜¾å­˜çš„å ç”¨ã€‚
> 
> ä¸ºäº†æ‰¾åˆ°æœ€ä¼˜çš„offloadç­–ç•¥ï¼Œä½œè€…å°†æ¨¡å‹è®­ç»ƒè¿‡ç¨‹çœ‹ä½œæ•°æ®æµå›¾ï¼ˆdata-flow graphï¼‰ã€‚åœ†å½¢èŠ‚ç‚¹è¡¨ç¤ºæ¨¡å‹çŠ¶æ€ï¼Œæ¯”å¦‚å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€çŸ©å½¢èŠ‚ç‚¹è¡¨ç¤ºè®¡ç®—æ“ä½œï¼Œæ¯”å¦‚å‰å‘è®¡ç®—ã€åå‘è®¡ç®—å’Œå‚æ•°æ›´æ–°è¾¹è¡¨ç¤ºæ•°æ®æµå‘ä¸‹å›¾æ˜¯æŸä¸€å±‚çš„ä¸€æ¬¡è¿­ä»£è¿‡ç¨‹ï¼ˆiteration/stepï¼‰ï¼Œä½¿ç”¨äº†æ··åˆç²¾è¯»è®­ç»ƒï¼Œå‰å‘è®¡ç®—ï¼ˆFWDï¼‰éœ€è¦ç”¨åˆ°ä¸Šä¸€æ¬¡çš„æ¿€æ´»å€¼ï¼ˆactivationï¼‰å’Œæœ¬å±‚çš„å‚æ•°ï¼ˆparameterï¼‰ï¼Œåå‘ä¼ æ’­ï¼ˆBWDï¼‰ä¹Ÿéœ€è¦ç”¨åˆ°æ¿€æ´»å€¼å’Œå‚æ•°è®¡ç®—æ¢¯åº¦ï¼Œ
> ![1680182200791](../images/2023-03-25-llm-singlegpu/1680182200791.png)
> ZeRO-Offloadçš„åˆ‡åˆ†æ€è·¯æ˜¯ï¼šå›¾ä¸­æœ‰å››ä¸ªè®¡ç®—ç±»èŠ‚ç‚¹ï¼šFWDã€BWDã€Param updateå’Œfloat2halfï¼Œå‰ä¸¤ä¸ªè®¡ç®—å¤æ‚åº¦å¤§è‡´æ˜¯ $O(MB)$ï¼Œ$B$ æ˜¯batch sizeï¼Œåä¸¤ä¸ªè®¡ç®—å¤æ‚åº¦æ˜¯ $O(M)$ã€‚ä¸ºäº†ä¸é™ä½è®¡ç®—æ•ˆç‡ï¼Œå°†å‰ä¸¤ä¸ªèŠ‚ç‚¹æ”¾åœ¨GPUï¼Œåä¸¤ä¸ªèŠ‚ç‚¹ä¸ä½†è®¡ç®—é‡å°è¿˜éœ€è¦å’ŒAdamçŠ¶æ€æ‰“äº¤é“ï¼Œæ‰€ä»¥æ”¾åœ¨CPUä¸Šï¼ŒAdamçŠ¶æ€è‡ªç„¶ä¹Ÿæ”¾åœ¨å†…å­˜ä¸­ï¼Œä¸ºäº†ç®€åŒ–æ•°æ®å›¾ï¼Œå°†å‰ä¸¤ä¸ªèŠ‚ç‚¹èåˆæˆä¸€ä¸ªèŠ‚ç‚¹FWD-BWD Super Nodeï¼Œå°†åä¸¤ä¸ªèŠ‚ç‚¹èåˆæˆä¸€ä¸ªèŠ‚ç‚¹Update Super Nodeã€‚å¦‚ä¸‹å›¾å³è¾¹æ‰€ç¤ºï¼Œæ²¿ç€gradient 16å’Œparameter 16ä¸¤æ¡è¾¹åˆ‡åˆ†ã€‚
> ![1680182405358](../images/2023-03-25-llm-singlegpu/1680182405358.png)
> ç°åœ¨çš„è®¡ç®—æµç¨‹æ˜¯ï¼Œåœ¨GPUä¸Šé¢è¿›è¡Œå‰å‘å’Œåå‘è®¡ç®—ï¼Œå°†æ¢¯åº¦ä¼ ç»™CPUï¼Œè¿›è¡Œå‚æ•°æ›´æ–°ï¼Œå†å°†æ›´æ–°åçš„å‚æ•°ä¼ ç»™GPUã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œå¯ä»¥å°†è®¡ç®—å’Œé€šä¿¡å¹¶è¡Œèµ·æ¥ï¼ŒGPUåœ¨åå‘ä¼ æ’­é˜¶æ®µï¼Œå¯ä»¥å¾…æ¢¯åº¦å€¼å¡«æ»¡bucketåï¼Œä¸€éè®¡ç®—æ–°çš„æ¢¯åº¦ä¸€éå°†bucketä¼ è¾“ç»™CPUï¼Œå½“åå‘ä¼ æ’­ç»“æŸï¼ŒCPUåŸºæœ¬ä¸Šå·²ç»æœ‰æœ€æ–°çš„æ¢¯åº¦å€¼äº†ï¼ŒåŒæ ·çš„ï¼ŒCPUåœ¨å‚æ•°æ›´æ–°æ—¶ä¹ŸåŒæ­¥å°†å·²ç»è®¡ç®—å¥½çš„å‚æ•°ä¼ ç»™GPUï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
> ![1680182460230](../images/2023-03-25-llm-singlegpu/1680182460230.png)


## **Orca**
- ğŸ”²[https://www.usenix.org/conference/osdi22/presentation/yu](https://www.usenix.org/conference/osdi22/presentation/yu)

- ğŸ”²[https://www.usenix.org/system/files/osdi22-yu.pdf](https://www.usenix.org/system/files/osdi22-yu.pdf)
- ğŸ”²[OSDI 2022 è®ºæ–‡è¯„è¿°-0x6ï¼šMachine Learning 2 - IPADS-SYSçš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/541704684)
> æœ¬é¡¹å·¥ä½œæå‡ºäº†é¢ç°å®æ—¶åœºæ™¯çš„DNNæ¨ç†è°ƒåº¦ç³»ç»ŸREEFï¼ŒREEFä¸»è¦æå‡ºäº†ä¸¤ä¸ªæ ¸å¿ƒæŠ€æœ¯ï¼šé¦–å…ˆï¼ŒåŸºäºDNNæ¨ç†ä»»åŠ¡GPU kernelçš„å¹‚ç­‰æ€§ï¼Œæå‡ºäº†reset-based preemptionæŠ€æœ¯ï¼Œé€šè¿‡ç›´æ¥killæ­£åœ¨æ‰§è¡Œçš„kernelï¼Œå®ç°äº†å¾®ç§’çº§çš„DNNæ¨ç†ä»»åŠ¡GPUæŠ¢å ï¼›å…¶æ¬¡ï¼ŒåŸºäºDNNæ¨ç†ä»»åŠ¡GPU kernelçš„æ—¶å»¶å¯é¢„æµ‹æ€§ï¼Œæå‡ºäº†dynamic kernel paddingæŠ€æœ¯ï¼Œå…è®¸RTå’ŒBEä»»åŠ¡å¹¶è¡Œæ‰§è¡Œï¼ŒåŒæ—¶å¯ä»¥ä¿è¯RTä»»åŠ¡çš„æ—¶å»¶ä¸å—å½±å“ã€‚
> ![1680183833147](../images/2023-03-25-llm-singlegpu/1680183833147.png)

## **FasterTransformer**
- ğŸ”²[è‹±ä¼Ÿè¾¾Fastertransformeræºç è§£è¯»](https://zhuanlan.zhihu.com/p/79528308)
- ğŸ”²[https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
- ğŸ”²[NVIDIA BERTæ¨ç†è§£å†³æ–¹æ¡ˆFaster Transformerå¼€æºå•¦](https://mp.weixin.qq.com/s/77mh--Z2dUbz6sTncNZIYA)
>ä¸ºäº†å‡å°‘kernelè°ƒç”¨æ¬¡æ•°ï¼Œå°†é™¤äº†çŸ©é˜µä¹˜æ³•çš„kerneléƒ½å°½å¯èƒ½åˆå¹¶é’ˆå¯¹å¤§batchå•ç‹¬è¿›è¡Œäº†kernelä¼˜åŒ–æ”¯æŒé€‰æ‹©æœ€ä¼˜çš„çŸ©é˜µä¹˜æ³•åœ¨ä½¿ç”¨FP16æ—¶ä½¿ç”¨half2ç±»å‹ï¼Œè¾¾åˆ°halfä¸¤å€çš„è®¿å­˜å¸¦å®½å’Œè®¡ç®—ååä¼˜åŒ–geluã€softmaxã€layernormçš„å®ç°ä»¥åŠé€‰ç”¨rsqrtç­‰



## **LightSeq**
- ğŸ”²[https://arxiv.org/abs/2010.13887](https://arxiv.org/abs/2010.13887)
- ğŸ”²[ç®€å•è¯»è¯»LightSeq - æˆ‘ä¸æ˜¯zzkçš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/398753306)
> ç±»ä¼¼LightSeqçš„é«˜æ€§èƒ½åŠ é€Ÿåº“ä¹Ÿæœ‰å¾ˆå¤šï¼Œä¸‹é¢çš„ä¸‰ä¸ªä¸»è¦ç‰¹æ€§æ˜¯æˆ‘ä»¬æ¯”åˆ«çš„åŠ é€Ÿåº“è¡¨ç°å¥½çš„åŸå› ï¼š
> - æˆ‘ä»¬å°†Tensorflow/Pytorchå®ç°ä¸­çš„ä¸€äº›ç»†ç²’åº¦Kernelï¼Œè¿›ä¸€æ­¥èåˆå®ç°æˆä¸€ä¸ªç²—ç²’åº¦çš„Kernelï¼Œä»è€Œé¿å…å¤§é‡æ ¸å‡½æ•°å¯åŠ¨å’ŒGPU memory IOå¸¦æ¥çš„æ—¶é—´æˆæœ¬
> æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§hierarchicalï¼ˆå±‚çº§ï¼‰ auto regressive searchæ¥æ›¿ä»£auto regressive searchï¼Œè¿›ä¸€æ­¥åŠ é€Ÿ
> æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€æ˜¾å­˜å¤ç”¨ç­–ç•¥ï¼Œåœ¨NLPå¤„ç†ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°å˜é•¿æ•°æ®ï¼Œç»™å†…å­˜åˆ†é…å¸¦æ¥äº†å›°éš¾ã€‚LightSeqé¢„å…ˆå®šä¹‰äº†æ¯ä¸ªkernelæœ€å¤§å¯ä½¿ç”¨æ˜¾å­˜ï¼Œå¹¶ç»™ä¸å­˜åœ¨ä¾èµ–å…³ç³»çš„kernelè¿›è¡Œå…±äº«ï¼Œèƒ½å¤Ÿå‡å°‘8å€å†…å­˜åˆ†é…ã€‚


## **TurboTransformers**
- ğŸ”²[https://arxiv.org/abs/2010.05680](https://arxiv.org/abs/2010.05680)
- ğŸ”²[å¾®ä¿¡ä¹Ÿåœ¨ç”¨çš„TransformeråŠ é€Ÿæ¨ç†å·¥å…·ï¼Œç°åœ¨è…¾è®¯å¼€æºäº† - é‡å­ä½çš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/136194893)
> ç®—å­å±‚ä¼˜åŒ–
> Transformeréƒ½åŒ…å«äº†ä»€ä¹ˆè®¡ç®—å‘¢ï¼Ÿ
> å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå›¾(a)å±•ç¤ºäº†è®ºæ–‡Transformerç»“æ„ç¤ºæ„å›¾ï¼Œè¿™é‡Œç§°ç°è‰²æ–¹æ¡†å†…çš„ç»“æ„ä¸ºä¸€ä¸ªTransformer Cellï¼ŒBERT encoderå †å äº†Nxä¸ªè¿™æ ·çš„Transformer Cellã€‚å›¾(b)å°†ä¸€ä¸ªCellçš„ç»†èŠ‚åŠ ä»¥å±•å¼€ï¼Œæ¯ä¸€ä¸ªçŸ©å½¢éƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„è®¡ç®—æ ¸å¿ƒã€‚
> ![1680185800053](../images/2023-03-25-llm-singlegpu/1680185800053.png)
> Transformer Cellè®¡ç®—åŒ…å«äº†8ä¸ªGEMM(é€šç”¨çŸ©é˜µä¹˜æ³•ï¼ŒGeneral Matrix Multiplication)è¿ç®—ã€‚é€šè¿‡è°ƒä¼˜Intel MKLå’ŒcuBLASçš„GEMMè°ƒç”¨æ–¹å¼æ¥è·å¾—æœ€ä½³GEMMæ€§èƒ½ã€‚å¹¶ä¸”åœ¨ç¡¬ä»¶å…è®¸æ¡ä»¶ä¸‹ï¼Œåœ¨GPUä¸Šä½¿ç”¨tensor coreæ–¹å¼è¿›è¡ŒGEMMè¿ç®—ã€‚
> ç±»ä¼¼NVIDIA FasterTransformersæ–¹æ¡ˆï¼Œå°†æ‰€æœ‰GEMMè¿ç®—ä¹‹é—´çš„è®¡ç®—èåˆæˆä¸€ä¸ªè°ƒç”¨æ ¸å¿ƒã€‚èåˆä¼šå¸¦æ¥ä¸¤ä¸ªå¥½å¤„ï¼Œä¸€æ˜¯å‡å°‘äº†å†…å­˜è®¿é—®å¼€é”€ï¼ŒäºŒæ˜¯å‡å°‘å¤šçº¿ç¨‹å¯åŠ¨å¼€é”€ã€‚å¯¹äºè¿™äº›æ ¸å¿ƒï¼Œåœ¨CPUä¸Šé‡‡ç”¨openmpè¿›è¡Œå¹¶è¡Œï¼Œåœ¨GPUä¸Šä½¿ç”¨CUDAè¿›è¡Œä¼˜åŒ–å®ç°ã€‚å¯¹äºæ¯”è¾ƒå¤æ‚çš„LayerNormå’ŒSoftmaxç®—å­ï¼Œå®ƒä»¬åŒ…å«äº†ä¸é€‚åˆGPUä¸Šå¹¶è¡Œçš„è§„çº¦æ“ä½œï¼ŒTurboTransformersä¸ºå®ƒä»¬è®¾è®¡äº†åˆ›æ–°å¹¶è¡Œç®—æ³•ï¼Œæå¤§é™ä½äº†è¿™äº›ç®—å­çš„å»¶è¿Ÿã€‚ç†è®ºä¸ŠTransformersæ¨ç†å»¶è¿Ÿåº”è¯¥è¿‘ä¼¼äºçŸ©é˜µä¹˜æ³•å»¶è¿Ÿã€‚

## **Huggingface**
- ğŸ”²[https://arxiv.org/abs/1910.03771](https://arxiv.org/abs/1910.03771)
- ğŸ”² [https://huggingface.co/docs/accelerate/index](https://huggingface.co/docs/accelerate/index)
- 


---

> ä¸€ç±»å·¥ä½œæ˜¯é‡åŒ–ï¼ˆquantizationï¼‰å’Œç¨€ç–åŒ–ï¼ˆsparsificationï¼‰å·¥ä½œ

## âœ…[Flexgen](https://arxiv.org/pdf/2303.06865.pdf)

| System | OPT-6.7B | OPT-30B | OPT-175B |
| ------ | -------- | ------- | -------- |
| Hugging Face Accelerate  | 25.12 (2 on GPU)  | 0.62 (8 on CPU) | 0.01 (2 on disk) |
| DeepSpeed ZeRO-Inference | 9.28 (16 on CPU)  | 0.60 (4 on CPU) | 0.01 (1 on disk) |
| Petals                 | 8.25 (2 on GPU) | 2.84 (2 on GPU) | 0.08 (2 on GPU) |
| FlexGen                  | 25.26 (2 on GPU) | 7.32 (144 on CPU) | 0.69 (256 on disk) |
| FlexGen with Compression | **29.12** (72 on GPU) | **8.38** (512 on CPU) | **1.12** (144 on CPU) |


## ğŸ”²[OpenAI å·¥ç¨‹å¸ˆ lilianweng å…³äºLLMæ¨ç†è®¡ç®—ä¼˜åŒ–çš„æ–¹æ³•æ¦‚è¿°](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)

> Several methods can be used to make inference cheaper in memory or/and faster in time.
>
> 1. Apply various parallelism to scale up the model across a large number of GPUs. Smart parallelism of model components and data makes it possible to run a model of trillions of parameters. ï¼ˆæ‰¹æ³¨ï¼š ä¾‹å¦‚ï¼ŒDeepspeed inferenceå¯¹denseå’ŒMoE ç¨€ç–æ¨¡å‹çš„å¹¶è¡Œç­–ç•¥ï¼Œæé«˜å¹¶è¡Œåº¦ï¼Œé™ä½è®¡ç®—å»¶è¿Ÿï¼‰
> 2. Memory offloading to offload temporarily unused data to the CPU and read them back when needed later. This helps with memory usage but causes higher latency. ï¼ˆæ‰¹æ³¨ï¼š ä¾‹å¦‚ï¼Œflexgenå’ŒDeepspeed inferenceéƒ½æœ‰å¯¹CPU offloadçš„è®¾è®¡ï¼Œå…¶ä¸­flexgenæ›´æ˜¯å°†è¿™ç§è®¾è®¡æ¨å‘æ–°é«˜åº¦ï¼Œæ ¸å¿ƒè¿½æ±‚throughoutæ€§èƒ½ï¼Œé€‚åˆç¦»çº¿åœºæ™¯ï¼‰
> 3. Smart batching strategy; E.g. EffectiveTransformer packs consecutive sequences together to remove padding within one batch.
> 4. Network compression techniques, such as pruning, quantization, distillation. A model of smaller size, in terms of parameter count or bitwidth, should demand less memory and run faster. ï¼ˆæ‰¹æ³¨ï¼š ä¸€èˆ¬ç‰ºç‰²ç²¾åº¦æˆ–è€…éœ€è¦å¾®è°ƒï¼Œåœ¨å°æ¨¡å‹æ—¶ä»£ç«¯ä¾§æ¨ç†å¸¸ç”¨çš„æ–¹æ³•ï¼Œç°åœ¨åœ¨å¤§æ¨¡å‹æ—¶ä»£ï¼Œè¿™ç§ç±»ä¼¼ä¼˜åŒ–æŠ€å·§åœ¨äº‘ç«¯ä¹Ÿæ›´åŠ é‡è§†ï¼‰
> 5. Improvement specific to a target model architecture. Many architectural changes, especially those for attention layers, help with transformer decoding speed. ï¼ˆæ‰¹æ³¨ï¼š Large Transformer Model Inference Optimization é‡ç‚¹æ€»ç»“äº†æ¨¡å‹ç»“æ„æ–¹é¢çš„ä¼˜åŒ–è®¾è®¡ï¼Œä¾‹å¦‚ï¼Œå›´ç»•Transformeræ¨¡å‹ç»“æ„è¿›è¡Œè®¡ç®—ä¼˜åŒ–ï¼Œé™ä½ç†è®ºè®¡ç®—é‡ï¼Œä¸æ˜¯è®¡ç®—æœºç³»ç»Ÿæ–¹é¢çš„ä¼˜åŒ–è®¾è®¡äº†ï¼‰
