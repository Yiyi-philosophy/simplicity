---
title: 'Read List of LLM'
image: '2023-03-25-llm-singlegpu/1679629332417.gif'
tags: Explore LLM
---
<!--more-->

# Sourse

## [ç‹ç‡•é£](https://www.zhihu.com/people/nono-nono-66/posts)

- âœ…[å¤§æ¨¡å‹LLMsç®—æ³•å’Œè®¡ç®—ç³»ç»Ÿé‡ç‚¹è®ºæ–‡ - AIç³»ç»Ÿå·¥ç¨‹å¸ˆè§†è§’Reading List ï¼ˆ1ï¼‰ - ç‹ç‡•é£çš„æ–‡ç« ](https://zhuanlan.zhihu.com/p/608268806)

- âœ…[å¤§æ¨¡å‹LLMsç®—æ³•å’Œè®¡ç®—ç³»ç»Ÿé‡ç‚¹è®ºæ–‡ - AIç³»ç»Ÿå·¥ç¨‹å¸ˆè§†è§’Reading List ï¼ˆ2ï¼‰ - ç‹ç‡•é£çš„æ–‡ç« ](https://zhuanlan.zhihu.com/p/616200081)

### Flexgen

- âœ…[å•å¡é«˜ååçš„å¤§è¯­è¨€æ¨¡å‹æ¨ç† - å˜»å˜»å˜»çš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/608279479)

- âœ…[Flexgen LLMæ¨ç† CPU Offloadè®¡ç®—æ¶æ„åˆ°åº•å¹²äº†ä»€ä¹ˆäº‹æƒ…ï¼Ÿ - ç‹ç‡•é£çš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/615021309)

- âœ…[Flexgen LLMæ¨ç†è®¡ç®—ç¯èŠ‚çš„é‡åŒ–åˆ†æ - ç‹ç‡•é£çš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/615327112)

- âœ…[Flexgen LLMæ¨ç†ç›¸å…³å·¥ä½œ - æ€ä¹ˆæ€è€ƒå¯»æ‰¾ä¼˜åŒ–æ–¹æ³• - ç‹ç‡•é£çš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/615328081)

- ğŸ”²[DeepSpeedä¹‹ZeROç³»åˆ—ï¼šå°†æ˜¾å­˜ä¼˜åŒ–è¿›è¡Œåˆ°åº• - basicv8vcçš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/513571706)

## Muli

### Methods

- âœ…[**å¦‚ä½•è¯»è®ºæ–‡**ï¼š](https://www.bilibili.com/video/BV1H44y1t75x/)

- ğŸ”²[å¦‚ä½•åˆ¤æ–­ï¼ˆä½ è‡ªå·±çš„ï¼‰ç ”ç©¶å·¥ä½œçš„ä»·å€¼- [è®ºæ–‡ç²¾è¯»]](https://www.bilibili.com/video/BV1oL411c7Us/)

- ğŸ”²[ä½ ï¼ˆè¢«ï¼‰åæ§½è¿‡è®ºæ–‡ä¸å¤Ÿ novel å—ï¼Ÿ- [è®ºæ–‡ç²¾è¯»]](https://www.bilibili.com/video/BV1ea41127Bq/)

### Background Knowledge

- âœ…[GPTï¼ŒGPT-2ï¼ŒGPT-3 è®ºæ–‡ç²¾è¯»- [è®ºæ–‡ç²¾è¯»]](https://www.bilibili.com/video/BV1AF411b7xQ/)

- âœ…[Transformerè®ºæ–‡é€æ®µç²¾è¯»- [è®ºæ–‡ç²¾è¯»]](https://www.bilibili.com/video/BV1pu411o7BE/)

- ğŸ”²[Megatron LM è®ºæ–‡ç²¾è¯»- [è®ºæ–‡ç²¾è¯»]](https://www.bilibili.com/video/BV1nB4y1R7Yz/)

- ğŸ”²[Zero è®ºæ–‡ç²¾è¯»[è®ºæ–‡ç²¾è¯»]](https://www.bilibili.com/video/BV1tY411g7ZT/)

- ğŸ”²[InstructGPT è®ºæ–‡ç²¾è¯»[è®ºæ–‡ç²¾è¯»Â·48]](https://www.bilibili.com/video/BV1hd4y187CR/)

- ğŸ”²[HELM å…¨é¢è¯­è¨€æ¨¡å‹è¯„æµ‹[è®ºæ–‡ç²¾è¯»Â·50]](https://www.bilibili.com/video/BV1z24y1B7uX/?share_source=copy_web&vd_source=3d7484df4d1e9f698fc8dd9d979e07fc)

- ğŸ”²[Anthropic LLM è®ºæ–‡ç²¾è¯»[è®ºæ–‡ç²¾è¯»Â·51]](https://www.bilibili.com/video/BV1XY411B7nM/)

----

# **Article**

- âœ…[How to Read a Paper](https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf)

> å› ä¸ºLLMåº”ç”¨æ•ˆæœæ˜¾è‘—ï¼ˆOpenAI chatGPT/GPT-4ã€meta OPT/LLaMAï¼‰ï¼Œæ‰€ä»¥LLMçš„è®¡ç®—ç³»ç»Ÿç ”ç©¶å·¥ä½œä»ä¸“æ³¨è®­ç»ƒç³»ç»Ÿè®¾è®¡åˆ°æ¨ç†ç³»ç»Ÿæ–¹å‘æ‹“å±•ï¼Œç›®æ ‡é™ä½LLMæ¨ç†æˆæœ¬ã€æ¨ç†é—¨æ§›ï¼ŒFlexgenæåˆ°äº†å‡ ä¸ªé‡è¦å·¥ä½œï¼Œè°·æ­Œä»£è¡¨æ€§å·¥ä½œ PaLM inference ã€å¾®è½¯ä»£è¡¨æ€§å·¥ä½œ   Deepspeed-Inferenceï¼Œå¦å¤–OSDI22å¹´ Orcaå·¥ä½œã€‚PaLM inferenceå’ŒDeepspeed-Inferenceæ˜¯ç«¯åˆ°ç«¯ç³»ç»Ÿå¹¶è¡Œç³»ç»Ÿè®¾è®¡ï¼›Orcaä¾§é‡å¯¹å˜æˆseqçš„è®¡ç®—æ•ˆç‡æå‡ï¼Œè®¾è®¡token levelçš„å¹¶è¡Œè®¡ç®—ç³»ç»Ÿï¼Œä¾§é‡ç³»ç»Ÿè®¾è®¡ï¼ˆä¸å•çº¯æ˜¯å¤šGPUåŒæ„ç³»ç»Ÿä¸Šçš„é«˜æ€§èƒ½è®¡ç®—çš„å¹¶è¡Œè®¾è®¡ï¼Œæ›´å¼ºè°ƒç³»ç»ŸåŒ–çš„è®¾è®¡ï¼ŒåŒ…å«æ¨ç†è®¡ç®—ã€è°ƒåº¦æœåŠ¡ï¼‰æ¥æå‡æ¨ç†è®¡ç®—çš„èµ„æºåˆ©ç”¨ç‡ã€‚å…¶ä»–è¿˜æœ‰ï¼ŒFasterTransformerã€LightSeqã€TurboTransformers ã€  Huggingfaceçš„Accelerateå·¥ä½œã€‚è¿™ç±»å·¥ä½œæ¯”è¾ƒä¾§é‡ä»è®¡ç®—æœºç³»ç»Ÿè®¾è®¡è§’åº¦ç ”ç©¶å¯¹LLMæ¨ç†è®¡ç®—çš„ä¼˜åŒ–ï¼Œå¤§éƒ¨åˆ†ä¼˜åŒ–å…·æœ‰æ— æŸç‰¹æ€§ï¼Œç›¸å¯¹é€šç”¨æ€§æ›´é«˜ï¼ˆä½†æ˜¯éƒ¨åˆ†å·¥ä½œé‡Œä¹Ÿå­˜åœ¨æ­£äº¤çš„æœ‰æŸä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿç³»ç»Ÿæ€§èƒ½æå‡ï¼‰ã€‚

## **PaLM inference**
- ğŸ”²[Blog](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)
![1679629332417](../images/2023-03-25-llm-singlegpu/1679629332417.gif)
- ğŸ”²[PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf)

## **Deepspeed-Inference**
- ğŸ”²[https://www.deepspeed.ai/inference/](https://www.deepspeed.ai/inference/)
DeepSpeed Inference: Enabling Efficient Inference
of Transformer Models at Unprecedented Scale
- ğŸ”²[https://arxiv.org/pdf/2207.00032.pdf](https://arxiv.org/pdf/2207.00032.pdf)

## **Orca**
- ğŸ”²[https://www.usenix.org/conference/osdi22/presentation/yu](https://www.usenix.org/conference/osdi22/presentation/yu)

- ğŸ”²[https://www.usenix.org/system/files/osdi22-yu.pdf](https://www.usenix.org/system/files/osdi22-yu.pdf)

## **FasterTransformer**
- ğŸ”²[è‹±ä¼Ÿè¾¾Fastertransformeræºç è§£è¯»](https://zhuanlan.zhihu.com/p/79528308)
- ğŸ”²[https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)

## **LightSeq**
- ğŸ”²[https://arxiv.org/abs/2010.13887](https://arxiv.org/abs/2010.13887)

## **TurboTransformers**
- ğŸ”²[https://arxiv.org/abs/2010.05680](https://arxiv.org/abs/2010.05680)

## **Huggingface**
- ğŸ”²[https://arxiv.org/abs/1910.03771](https://arxiv.org/abs/1910.03771)

---

> ä¸€ç±»å·¥ä½œæ˜¯é‡åŒ–ï¼ˆquantizationï¼‰å’Œç¨€ç–åŒ–ï¼ˆsparsificationï¼‰å·¥ä½œ

- âœ…[Flexgen](https://arxiv.org/pdf/2303.06865.pdf)

- ğŸ”²[OpenAI å·¥ç¨‹å¸ˆ lilianweng å…³äºLLMæ¨ç†è®¡ç®—ä¼˜åŒ–çš„æ–¹æ³•æ¦‚è¿°](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)

> Several methods can be used to make inference cheaper in memory or/and faster in time.
>
> 1. Apply various parallelism to scale up the model across a large number of GPUs. Smart parallelism of model components and data makes it possible to run a model of trillions of parameters. ï¼ˆæ‰¹æ³¨ï¼š ä¾‹å¦‚ï¼ŒDeepspeed inferenceå¯¹denseå’ŒMoE ç¨€ç–æ¨¡å‹çš„å¹¶è¡Œç­–ç•¥ï¼Œæé«˜å¹¶è¡Œåº¦ï¼Œé™ä½è®¡ç®—å»¶è¿Ÿï¼‰
> 2. Memory offloading to offload temporarily unused data to the CPU and read them back when needed later. This helps with memory usage but causes higher latency. ï¼ˆæ‰¹æ³¨ï¼š ä¾‹å¦‚ï¼Œflexgenå’ŒDeepspeed inferenceéƒ½æœ‰å¯¹CPU offloadçš„è®¾è®¡ï¼Œå…¶ä¸­flexgenæ›´æ˜¯å°†è¿™ç§è®¾è®¡æ¨å‘æ–°é«˜åº¦ï¼Œæ ¸å¿ƒè¿½æ±‚throughoutæ€§èƒ½ï¼Œé€‚åˆç¦»çº¿åœºæ™¯ï¼‰
> 3. Smart batching strategy; E.g. EffectiveTransformer packs consecutive sequences together to remove padding within one batch.
> 4. Network compression techniques, such as pruning, quantization, distillation. A model of smaller size, in terms of parameter count or bitwidth, should demand less memory and run faster. ï¼ˆæ‰¹æ³¨ï¼š ä¸€èˆ¬ç‰ºç‰²ç²¾åº¦æˆ–è€…éœ€è¦å¾®è°ƒï¼Œåœ¨å°æ¨¡å‹æ—¶ä»£ç«¯ä¾§æ¨ç†å¸¸ç”¨çš„æ–¹æ³•ï¼Œç°åœ¨åœ¨å¤§æ¨¡å‹æ—¶ä»£ï¼Œè¿™ç§ç±»ä¼¼ä¼˜åŒ–æŠ€å·§åœ¨äº‘ç«¯ä¹Ÿæ›´åŠ é‡è§†ï¼‰
> 5. Improvement specific to a target model architecture. Many architectural changes, especially those for attention layers, help with transformer decoding speed. ï¼ˆæ‰¹æ³¨ï¼š Large Transformer Model Inference Optimization é‡ç‚¹æ€»ç»“äº†æ¨¡å‹ç»“æ„æ–¹é¢çš„ä¼˜åŒ–è®¾è®¡ï¼Œä¾‹å¦‚ï¼Œå›´ç»•Transformeræ¨¡å‹ç»“æ„è¿›è¡Œè®¡ç®—ä¼˜åŒ–ï¼Œé™ä½ç†è®ºè®¡ç®—é‡ï¼Œä¸æ˜¯è®¡ç®—æœºç³»ç»Ÿæ–¹é¢çš„ä¼˜åŒ–è®¾è®¡äº†ï¼‰
