---
title : ' Linear Attention [1/n]'
image : '2025-12-12-RNN/image-20251212115350633.png'
tag : RNN
---

# Linear Attention [1/n]

![image-20251212115350633](../images/2025-12-12-RNN/image-20251212115350633.png)

![img](../images/2025-12-12-RNN/model_architecture.png)



<!--more-->

---
2025-12-10

[DeepSeek-V3.2-Exp 和 Qwen3-Next 哪个才是未来？ - sonta的回答 - 知乎](https://www.zhihu.com/question/1956137082197083536/answer/1956300827774935213)

- 很高质量的回答！Sparse + Linear Attention 合理结合后确实是可以Work的，正好借机推荐一篇工作做[Sparse-Linear Attention:](https://www.arxiv.org/abs/2509.24006)
- 因为理论和实验上Softmax Attention的Attention Weight刚好可以被完美分解为一个很高稀疏度的，但Rank很高的矩阵，和一个非常不稀疏，但是Rank很低的矩阵（[Kelvin-Helmholtz instability in binary fluids with miscibility gap](https://arxiv.org/abs/2509.02400)中分析的）。这样完美对应了Sparse-Linear Attention。

---
2025-12-11

[Ultra-Sparse Memory Network](http://arxiv.org/abs/2411.12364)
[UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning](http://arxiv.org/abs/2508.18756)

UltraMem V1 
1. 在 Transformer 里插入**多层小型稀疏记忆层**，替代 PKM 那种单一巨大内存层，查询更容易命中、有利于大规模扩展。
2. 用 **Tucker 分解 + TDQKR 检索 + IVE 隐式值扩展**，在不明显增加显存的前提下，大幅增加“虚拟记忆容量”。
3. 相比 MoE，**几乎不需要跨设备通信**，训练稳定、推理速度更接近 Dense 模型，但参数规模可以做得很大。

UltraMem V2
1. 把 UltraMem 深度融合进 **每个 Transformer block（FFN+UltraMemV2）**，让模型在所有层都拥有稀疏记忆能力，下游任务效果明显优于 V1。
2. 通过 **共享 memory 的 Ring/Block 拓扑**，不同层复用 value 表，既扩大记忆规模又控制住访存和通信开销。
3. 升级版 IVE + **PEER 风格“小型 FFN value”**，把每个 memory value 变成微型 expert，本质上是“更细粒度、更稳定的 MoE 式记忆层”。

---
2025-12-12

[线性注意力简史：从模仿、创新到反哺](https://spaces.ac.cn/archives/11033)

1. Linear RNN / 线性注意力 去掉了 Softmax Attention，使得对序列长度的时间和空间复杂度从 O(n²) 降为 O(n)（对长度线性、可递推）。
2. TTT（Test-Time Training）视角 将上下文缓存（memory / cache）不再显式存储，而是通过梯度更新把历史信息压缩进模型权重中。
3. DeltaNet 代表了一类主流 RNN 思路，其核心是主动遗忘过去信息、用更新规则持续注入新认知，而不是无界累积历史状态。

