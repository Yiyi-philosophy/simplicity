---
title : ' RL algorithm: from PPO to GRPO and DAPO'
image : '2025-05-26-PPO/image-20250526124841741.png'
tag : RL
---

# RL algorithm: from PPO to GRPO and DAPO

> https://zhuanlan.zhihu.com/p/1898817630208517687

<!--more-->

![image-20250526124841741](../images/2025-05-26-PPO/image-20250526124841741.png)

## 1. Main concepts

- On-Policy: training data produced from policy which interacting with environment, which means it needs real-time interaction.
-  Off-Policy: training data produced from some prepared policy environment.

PPO, GRPO, DAPO are On-policy strategies, which contain four key compounds:

- Actor:  produce action policy
- Critic: evaluate the value network of action and situation
- Reward Model: model or function that give the real-time reward for situation transfer
- Reference Model: reference strategies, in order to prevent the bias between updated policy network and original policy network.

some algorithms:

On-Policy
- Monte Carlo Policy Gradient
- Actor-Critic
- REINFORCE
- Trust Region Policy Optimization (TRPO)
- Proximal Policy Optimization (PPO)

Off-Policy
- Q-Learning
- SARSA
- Deep Q-Network (DQN)

## 2. PPO

### Policy Gradient Optimization

The goal is to maximize the expected reward, the target can be written as:
$$
\pi^\star = \arg\max_\pi J(\pi)
$$
And reward form each real-time reward in trajectory, so the target can also be written as:
$$
J(\pi_\theta) = \int_\tau {\color{green}P(\tau|\pi)}R(\tau) = \mathbb{E}_{\tau \sim \pi}[R(\tau)]
$$
in which $\tau$ presents a set of action-state trajectories, $\tau=(s_0,a_0,s_1,a_1 \cdots)$, and $R_t=\sum_{t=0}^\infty \gamma^t r_t$. in this situation, status transfer are decided by current situation and chosen decision: $s_{t+1}\sim P(\cdot \mid s_t,a_t) $.

A trajectory's total statistic distribution can be written as:
$$
{\color{green}P(\tau|\pi)} = \rho(s_0) \prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \pi(a_t, s_t)
$$
For the policy gradient method, we hope get the maximize trajectory reward by gradient ascent. the update gradient ascent formulation is:





Thus, $J(\theta)$ can be written as:  
$$ J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[\sum_{t=0}^T \gamma^t r_t\right] $$  

According to the policy gradient theorem, the gradient of $J(\theta)$ is:  
$$ \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[\sum_{t=0}^T \nabla \log \pi_\theta (a_t|s_t) R(\tau)\right] $$  

This formula can be used to update the policy parameters $\theta$. However, directly using this formula results in high variance in gradient estimates because $R(\tau)$ is a random variable.  

To reduce variance, we can introduce a baseline function $b(s_t)$:  
$$ \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[\sum_{t=0}^T \nabla \log \pi_\theta (a_t|s_t) (R(\tau) - b(s_t))\right] $$  

Typically, the baseline function $b(s_t)$ can be chosen as the state value function $V(s_t)$.  

Finally, the update rule for the policy gradient algorithm is:  
$$ \theta \leftarrow \theta + \alpha \nabla J(\theta) $$  

where $\alpha$ is the learning rate.  

### REINFORCE  
The REINFORCE algorithm is a policy gradient algorithm based on Monte Carlo sampling. It estimates the policy gradient by computing cumulative rewards over complete trajectories.  

The advantage of REINFORCE is its simplicity and the fact that it does not require modeling the environment. However, its drawback is high variance and slow convergence.  

In REINFORCE, the cumulative reward $R(\tau)$ of the entire trajectory is used to evaluate actions. However, this evaluation method leads to high variance because $R(\tau)$ is a random variable.  

To reduce variance, REINFORCE often introduces a baseline function, typically the state value function $V(s_t)$.  

$$ \nabla J(\theta) = \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^{T_i} \nabla \log \pi_\theta (a_{i,t}|s_{i,t}) (G_{i,t} - b(s_{i,t})) $$  

### Actor-Critic  
The Actor-Critic algorithm combines the strengths of policy gradients and value function estimation. The Actor part learns the policy, while the Critic part evaluates the policy.  

$$ \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $$  

The Actor part uses the advantage function estimated by the Critic part to update the policy.  

### Temporal Difference (TD) Error  
TD Error is a commonly used concept in Actor-Critic algorithms to evaluate the difference between the value of the current state and the next state.  

The formula for TD Error is:  
$$ \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $$  

Here, $r_t$ is the reward obtained at time step $t$, $\gamma$ is the discount factor, and $V(s_t)$ is the value function of state $s_t$.  

### Focus Only on the Future  
In some cases, we are more concerned with future rewards than past or current rewards. This can be achieved using the discount factor $\gamma$. In Actor-Critic algorithms, the Critic part typically uses TD Error to estimate the state value function or advantage function.  

$$ \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[\sum_{t=0}^T \nabla \log \pi_\theta (a_t|s_t) \sum_{k=t}^T \gamma^{k-t} r_k\right] $$  

### Advantage Function  
The advantage function is used in Actor-Critic algorithms to evaluate the value of actions. It measures the additional benefit of taking a specific action in a given state compared to other actions.  

$$ A(s_t, a_t) = Q(s_t, a_t) - V(s_t) $$  

The advantage function can be used to reduce variance in policy gradient algorithms. By introducing the advantage function, the policy gradient formula can be rewritten as:  
$$ \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[\sum_{t=0}^T \nabla \log \pi_\theta (a_t|s_t) A(s_t, a_t)\right] $$  

### Generalized Advantage Estimation (GAE)  
GAE is a general method for estimating the advantage function, combining the strengths of TD error and Monte Carlo estimation.  

The formula for GAE is:  
$$ \hat{A}_t^{GAE(\gamma, \lambda)} = \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{t+l} $$  

where $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ is the TD error.  

GAE balances variance and bias by introducing a decay factor $\lambda$.  

### Trust Region Policy Optimization (TRPO)  
TRPO is an optimization method based on policy gradients. It improves stability by limiting the extent of policy changes during updates.  

The objective of TRPO is to maximize the expected cumulative reward while constraining the distance between the new policy and the old policy.  

The optimization problem for TRPO can be expressed as:  
$$ \max_\theta \mathbb{E}_{s_t, a_t \sim \pi_{old}} \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{old}(a_t|s_t)} A(s_t, a_t) \right] $$  

subject to $\mathbb{E}_{s_t \sim \pi_{old}} [D_{KL}(\pi_\theta(\cdot|s_t) || \pi_{old}(\cdot|s_t))] \leq \delta$.  

Here, $D_{KL}$ is the KL divergence, and $\delta$ is the maximum allowed KL divergence.  

TRPO solves this constrained optimization problem using the conjugate gradient method and line search.  

### Proximal Policy Optimization (PPO)  
PPO is an improved version of TRPO. It simplifies the optimization process by using clipping and multi-step updates while maintaining the stability of TRPO.  

The objective function for PPO can be written as:  
$$ L^{CLIP}(\theta) = \mathbb{E}_{s_t, a_t \sim \pi_{old}} \left[ \min(r_t(\theta) A(s_t, a_t), \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A(s_t, a_t)) \right] $$  

where $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{old}(a_t|s_t)}$ is the importance sampling ratio, and $\epsilon$ is the clipping parameter.  

PPO limits the extent of policy changes by clipping the importance sampling ratio, avoiding instability caused by large policy updates.  

PPO's advantages include simplicity, computational efficiency, and strong performance across various tasks.  

$$ \text{clip}(x, \text{low}, \text{high}) = \max(\text{low}, \min(x, \text{high})) $$  

### PPO-Penalty  
PPO-Penalty is a variant of PPO that incorporates a KL divergence penalty term into the objective function to constrain policy changes.  

The objective function for PPO-Penalty is:  
$$ L^{KLPEN}(\theta) = \mathbb{E}_{s_t, a_t \sim \pi_{old}} \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{old}(a_t|s_t)} A(s_t, a_t) - \beta D_{KL}(\pi_\theta(\cdot|s_t) || \pi_{old}(\cdot|s_t)) \right] $$  

Here, $\beta$ is the penalty coefficient.  

PPO-Penalty dynamically adjusts the penalty coefficient $\beta$ to balance policy updates and KL divergence.  

$$ \beta_k = \begin{cases} \beta_0 / 1.5 & \text{if } D_{KL}^K > D_{KL}^{\text{target}} \times 1.5 \\ \beta_0 \times 1.5 & \text{if } D_{KL}^K < D_{KL}^{\text{target}} / 1.5 \\ \beta_0 & \text{otherwise} \end{cases} $$  

### Off-Policy  
Off-Policy means that the learning policy is different from the behavior policy used for sampling. This allows the reuse of old experience data, improving data efficiency.  

Importance sampling is a common technique in Off-Policy learning. It weights samples from different policies to estimate the expected value under the target policy.  

$$ \mathbb{E}_{x \sim p}[f(x)] = \mathbb{E}_{x \sim q} \left[ \frac{p(x)}{q(x)} f(x) \right] $$  

Here, $p(x)$ is the target distribution, and $q(x)$ is the sampling distribution.  

### Q-Learning  
Q-Learning is an Off-Policy algorithm based on value function estimation. It learns the optimal action-value function $Q(s,a)$ to find the optimal policy.  

$$ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)] $$  

### Deep Q-Network (DQN)  
DQN combines Q-Learning with deep neural networks, using a deep neural network to approximate the action-value function $Q(s,a)$.  

The objective function for DQN is:  
$$ L(\theta) = \mathbb{E}_{(s,a,r,s') \sim D} \left[ (r + \gamma \max_{a'} Q_{target}(s', a') - Q(s,a; \theta))^2 \right] $$  

DQN addresses the instability of deep neural networks in reinforcement learning using experience replay and target networks.  

Experience replay: Stores each time step's experience $(s_t, a_t, r_t, s_{t+1})$ in a replay buffer and randomly samples batches for training.  

Target network: Uses an independent network to compute target Q-values, whose parameters are periodically copied from the main network.  

### Advantage Actor-Critic (A2C)  
A2C is the synchronous parallel version of the Actor-Critic algorithm. It trains in multiple parallel environments and accumulates gradients from all environments to update the model.  

$$ J(\theta_v) = \mathbb{E}_{s_t \sim \rho^\pi} \left[(R_t - V(s_t;\theta_v))^2\right] $$  

$$ J(\theta_\pi) = \mathbb{E}_{s_t \sim \rho^\pi} \left[\log \pi(a_t|s_t;\theta_\pi) A(s_t,a_t)\right] $$  

### Asynchronous Advantage Actor-Critic (A3C)  
A3C is the asynchronous parallel version of A2C. It trains in multiple parallel environments, with each environment having its own Agent interacting independently and computing gradients. These gradients are then asynchronously used to update a globally shared model.  

A3C's advantages include computational efficiency and strong performance across various tasks.  

A3C's objective function is similar to A2C's but uses asynchronous updates.  

$$ J(\theta_v) = \mathbb{E}_{s_t \sim \rho^\pi} \left[(R_t - V(s_t;\theta_v))^2\right] $$  

$$ J(\theta_\pi) = \mathbb{E}_{s_t \sim \rho^\pi} \left[\log \pi(a_t|s_t;\theta_\pi) A(s_t,a_t) + \beta H(\pi(s_t;\theta_\pi))\right] $$  

Here, $H(\pi)$ is the policy's entropy, encouraging exploration.  

### Distributed PPO (DPPO)  
DPPO is the distributed version of PPO. It accelerates training by sharing model parameters across multiple workers and collecting experience data in parallel.  

DPPO's advantages include faster training and the ability to handle large-scale tasks.  

### DA-PPO  
DA-PPO is an improved PPO algorithm that enhances performance by introducing data augmentation and adaptive clipping.  

Data augmentation: Increases the diversity of training data by transforming experience data, such as random cropping or flipping.  

Adaptive clipping: Dynamically adjusts the clipping parameter $\epsilon$ based on the KL divergence during training to better balance policy updates and stability.  

## From PPO to DAPO  
Figure 1. Comparison of TRPO, PPO, and DAPO (Direct Preference Optimization) algorithm architectures.  

### PPO-Clip Algorithm  
The pseudocode for the PPO-Clip algorithm is as follows:  

| Algorithm 1 PPO-Clip Algorithm                               |
| :----------------------------------------------------------- |
| 1. Initialize policy network $\pi_\theta$ and value network $V_\phi$. |
| 2. For iteration $k=1, 2, \dots$ do                          |
| 3. Collect $N$ trajectories by running $\pi_{\theta_{old}}$ in the environment. |
| 4. Compute advantages $\hat{A}_t$ using GAE.                 |
| 5. For epoch $j=1, 2, \dots, K$ do                           |
| 6. Sample mini-batches from collected trajectories.          |
| 7. Compute ratio $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$. |
| 8. Compute clipped objective: $L^{CLIP}(\theta) = \mathbb{E}_{s_t,a_t \sim D} [\min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t)]$. |
| 9. Update policy network $\theta$ by maximizing $L^{CLIP}(\theta)$. |
| 10. Update value network $\phi$ by minimizing $(V_\phi(s_t) - R_t)^2$. |
| 11. End for                                                  |
| 12. Update $\theta_{old} \leftarrow \theta$.                 |
| 13. End for                                                  |

### Advantages Computation for PPO  
In PPO, the advantage function is typically computed using GAE (Generalized Advantage Estimation).  

A simple method for normalizing the advantage function is:  
$$ A_t = \frac{r_t - \text{mean}(\{r_1, r_2, \dots, r_N\})}{\text{std}(\{r_1, r_2, \dots, r_N\})} $$  

### PPO Parameter Tuning Tips  
PPO is relatively easy to debug, but some parameter tuning tips can improve training:  
1. Clipping parameter $\epsilon$: Commonly set to 0.2.  
2. Learning rate $\alpha$: Commonly set to $3 \times 10^{-4}$.  
3. Number of epochs for PPO update: Commonly set to 10.  
4. Batch size: Larger batch sizes generally lead to more stable training.  
5. GAE $\lambda$ parameter: Commonly set to 0.95.  
6. Value function coefficient $c_1$: Commonly set to 0.5.  
7. Entropy coefficient $c_2$: Commonly set to 0.01.  
8. Number of timesteps per rollout: Commonly set to 2048.  

### DAPO (Direct Preference Optimization)  
DAPO (Direct Preference Optimization) is a new reinforcement learning algorithm that learns policies by directly optimizing preferences, eliminating the need for explicit reward or value function estimation.  

The core idea of DAPO is to transform the policy optimization problem into a binary classification problem, avoiding the complex reward function design and value function estimation in traditional reinforcement learning algorithms.  

The objective function for DAPO is:  
$$ L(\pi) = \mathbb{E}_{(x, y_w, y_l) \sim D} \left[ -\log \sigma \left( \log \frac{\pi(y_w|x)}{\pi_{ref}(y_w|x)} - \log \frac{\pi(y_l|x)}{\pi_{ref}(y_l|x)} \right) \right] $$  

Here, $D$ is the preference dataset, $(x, y_w, y_l)$ represents a preference sample where $x$ is the context, $y_w$ is the preferred response, and $y_l$ is the non-preferred response. $\pi_{ref}$ is the reference policy.  

DAPO's advantages include simplicity, computational efficiency, and strong performance across various tasks.  

#### DAPO Algorithm  
1. Initialize policy network $\pi_\theta$ and reference policy $\pi_{ref}$.  
2. Collect preference data by comparing different responses.  
3. For iteration $k=1, 2, \dots$