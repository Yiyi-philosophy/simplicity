---
title : 'A (Long) Peek into Reinforcement Learning: Part3'
image : '2025-01-22-RL-intro/agent_environment_MDP.png'
tag : RL
---

# A (Long) Peek into Reinforcement Learning: Part3

<!--more-->

> [A (Long) Peek into Reinforcement Learning | Lil'Log](https://lilianweng.github.io/posts/2018-02-19-rl-overview/)

## Common Approaches

### Temporal-Difference Learning (TD)

- model-free
-  learns from episodes of experience (**incomplete** episodes)

#### Bootstrapping

- **bootstrapping**: existing estimate $\rightarrow$ update targets
- x need actual rewards and complete returns like MC

#### Value Estimation

- TD method: update $V(S_T)$ to estimated return (TD target ) $G_T = R_{t+1}+\gamma V(S_{t+1})$
  $$
  V(S_t)\leftarrow (1-\alpha)V(S_t)+\alpha G_t \\
  V(S_t)\leftarrow V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))
  $$
  
- action-value estimation

$$
Q ( S_{t}, A_{t} ) \gets Q ( S_{t}, A_{t} )+\alpha( R_{t+1}+\gamma Q ( S_{t+1}, A_{t+1} )-Q ( S_{t}, A_{t} ) )
$$

 TD learning (TD control)

#### SARSA: On-Policy TD control

- Update $Q$ following GPI

1. $t=0$

2. $S_0$ choose $A_0=\arg\max_{a\in\mathcal{A}}Q(S_0, a)$, common use $\epsilon$-greedy

3. At $t$, applying $A_t$, get $R_{t+1}$ and come to next state $S_{t+1}$

4. Similar to 2. $A_{t+1} = \arg\max_{a\in \mathcal{A}}Q(S_{t+1}, a)$ 

5. Update $Q$-value: 

   $Q ( S_{t}, A_{t} ) \gets Q ( S_{t}, A_{t} )+\alpha( R_{t+1}+\gamma Q ( S_{t+1}, A_{t+1} )-Q ( S_{t}, A_{t} ) )$

6. $t=t+1$, to 3.

#### Q-Learning: Off-policy TD control

1. $t=0, S_0$

2. At $t$, choose $A_t=\arg\max_{a\in\mathcal{A}}Q(S_t, a)$, common use $\epsilon$-greedy

3. At $t$, applying $A_t$, get $R_{t+1}$ and come to next state $S_{t+1}$

4. Update $Q$-value: 

   $Q ( S_{t}, A_{t} ) \gets Q ( S_{t}, A_{t} )+\alpha( R_{t+1}+\gamma \max_{a\in\mathcal{A}}Q ( S_{t+1}, a )-Q ( S_{t}, A_{t} ) )$

5. $t=t+1$, to 3.

Compared to SARSA, Q-learning do not follow current policy