---
title : ' Linear Attention [1/n]'
image : '2025-12-12-RNN/image-20251212115350633.png'
tag : RNN
---

# Linear Attention [1/n]

![image-20251212115350633](../images/2025-12-12-RNN/image-20251212115350633.png)

![img](../images/2025-12-12-RNN/model_architecture.png)



<!--more-->

---
2025-12-15

> PKM: 1907.05242
> MLS: 2412.09764
> UltraMem v1: 2411.12364
> UltraMem v2: 2508.18756

各个Mem架构横向比较
- PKM、UltraMem 和 MLS 在形式上都可写成 top-k 稀疏加权的 memory read，差别不在于是否用 top-k，而在于 routing 的数学地位。
- PKM 的 routing 来自 product key 的结构先验，是固定、非学习的；UltraMem v1/v2 把 routing 视为 检索或可学习的 gate；
- MLS 则进一步把 routing 明确写成 memory layer 的定义性原语（带约束的 mask），而不是实现技巧。

从统一视角看，前几者都可以视为 MLS 在不同 routing 约束下的特例。

